{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e57932",
   "metadata": {},
   "source": [
    "## This script loops through subfolders in 'cpics_img' to calculate deployment times for each CTD cast as sampling events in 'ctd_meta_v4.csv'.\n",
    "\n",
    "## Sometimes there are subfolders in 'cpics_img' that don't match any sampling event. These will be skipped.\n",
    "\n",
    "## Written by E. Montes (enrique.montes@noaa.gov)\n",
    "## November 3rd 2025\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a9961",
   "metadata": {},
   "source": [
    "**Workflow:**\n",
    "\n",
    "1.  **Load Metadata:** Read the event metadata CSV.\n",
    "2.  **Scan All Images:** Scan `cpics_img` and find *all* image timestamps, storing them in one large, sorted list.\n",
    "3.  **Process Events with Gap Detection:**\n",
    "      * For each event, define a large \"search window\" (from 10 min before the event to 20 min before the *next* event).\n",
    "      * Find all images in this window.\n",
    "      * Calculate the time differences *between* these images.\n",
    "      * Find the *first gap* larger than a threshold (e.g., 30 minutes).\n",
    "      * The deployment is the block of images *before* this first gap.\n",
    "4.  **Save Results:** Add the calculated durations to the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fafe957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c0625",
   "metadata": {},
   "source": [
    "### 1. Load Event Metadata\n",
    "\n",
    "First, we load the `ctd_meta_v4.csv` file, which contains the timestamp for each CTD cast. We'll parse the date and time columns into a single `datetime` object for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0be9d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded /home/enrique.montes@CNS.local/plankton_imaging/ctd_meta_v4.csv\n",
      "Data loaded and timestamps parsed:\n",
      "   year  month  day    time_gmt           timeStamp\n",
      "0  2022     12    4  1:01:00 AM 2022-12-04 01:01:00\n",
      "1  2022     12    4  2:27:00 AM 2022-12-04 02:27:00\n",
      "2  2022     12    4  3:23:00 AM 2022-12-04 03:23:00\n",
      "3  2022     12    4  4:45:00 AM 2022-12-04 04:45:00\n",
      "4  2022     12    4  6:35:00 AM 2022-12-04 06:35:00\n"
     ]
    }
   ],
   "source": [
    "# --- Load the CSV file ---\n",
    "csv_path = ('/home/enrique.montes@CNS.local/plankton_imaging/ctd_meta_v4.csv')\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(csv_path)\n",
    "    print(f\"Successfully loaded {csv_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find the CSV file at {csv_path}\")\n",
    "    data = pd.DataFrame() # Create empty dataframe\n",
    "    \n",
    "# --- Combine date and time components to create datetime object ---\n",
    "if not data.empty:\n",
    "    \n",
    "    # --- FIX: Drop rows with missing date/time components first ---\n",
    "    original_rows = len(data)\n",
    "    data = data.dropna(subset=['year', 'month', 'day', 'time_gmt'])\n",
    "    new_rows = len(data)\n",
    "    if original_rows > new_rows:\n",
    "        print(f\"Dropped {original_rows - new_rows} rows with missing date/time data.\")\n",
    "    \n",
    "    # --- Cast to int (which is now safe) and then to str ---\n",
    "    dateTimeStr = (data['year'].astype(int).astype(str) + '-' +\n",
    "                   data['month'].astype(int).astype(str) + '-' +\n",
    "                   data['day'].astype(int).astype(str) + ' ' +\n",
    "                   data['time_gmt'])\n",
    "\n",
    "    # Let pandas automatically infer the format.\n",
    "    data['timeStamp'] = pd.to_datetime(dateTimeStr) \n",
    "\n",
    "    print(\"Data loaded and timestamps parsed:\")\n",
    "    print(data[['year', 'month', 'day', 'time_gmt', 'timeStamp']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decf752",
   "metadata": {},
   "source": [
    "### Cell 3: Scan ALL Image Files\n",
    "\n",
    "Create one single, sorted list of all timestamps from all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11276941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Scan All Image Files\n",
    "parentDir = 'cpics_img' \n",
    "\n",
    "# This list will hold all valid timestamps from all matching files\n",
    "all_file_timestamps = []\n",
    "\n",
    "print(f\"Scanning all images in: {parentDir}\")\n",
    "\n",
    "for root, dirs, files in os.walk(parentDir):\n",
    "    # Filter out hidden directories\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "    \n",
    "    for file in files:\n",
    "        if file.startswith('20') and file.endswith('.png'):\n",
    "            try:\n",
    "                dateStr = file[0:15] # e.g., '20221204_010047'\n",
    "                file_timestamp = datetime.strptime(dateStr, '%Y%m%d_%H%M%S')\n",
    "                all_file_timestamps.append(file_timestamp)\n",
    "            except ValueError:\n",
    "                print(f\"Skipping file with unexpected name format: {file}\")\n",
    "\n",
    "# --- Convert to a sorted pandas Series for very fast processing ---\n",
    "if not all_file_timestamps:\n",
    "    print(\"Warning: No image files were found.\")\n",
    "    all_timestamps_series = pd.Series(dtype='datetime64[ns]')\n",
    "else:\n",
    "    all_timestamps_series = pd.Series(all_file_timestamps).sort_values().reset_index(drop=True)\n",
    "    print(f\"Found and processed {len(all_timestamps_series)} total image files.\")\n",
    "    print(\"\\nFirst 5 image timestamps found:\")\n",
    "    print(all_timestamps_series.head())\n",
    "    print(\"\\nLast 5 image timestamps found:\")\n",
    "    print(all_timestamps_series.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ea0eb",
   "metadata": {},
   "source": [
    "### Analyze Gap Distribution\n",
    "\n",
    "This cell plots a histogram of the time gaps between consecutive images.\n",
    "Look at the plot to find the natural break between \"intra-deployment\" gaps (left) and \"inter-deployment\" gaps (right).\n",
    "The empty space between them is your threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60361db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Gap Distribution\n",
    "\n",
    "if not all_timestamps_series.empty:\n",
    "    # 1. Calculate all gaps\n",
    "    gaps_in_seconds = all_timestamps_series.diff().dt.total_seconds()\n",
    "    gaps_in_minutes = gaps_in_seconds / 60.0\n",
    "\n",
    "    # 2. Plot a histogram of the log of the gaps (to see both small and large gaps)\n",
    "    # We filter out gaps <= 0 (e.g., first entry)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(gaps_in_minutes[gaps_in_minutes > 0].apply('log10'), bins=100)\n",
    "    plt.title('Histogram of Gaps Between Consecutive Images (Log Scale)')\n",
    "    plt.xlabel('Gap in Minutes (log10 scale)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Add labels for interpretation\n",
    "    # (e.g., 1 min = 0, 10 min = 1, 60 min = 1.78, 100 min = 2)\n",
    "    plt.gca().set_xticklabels([f\"{10**x:.1f} min\" for x in plt.gca().get_xticks()])\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- How to read this chart: ---\")\n",
    "    print(\"You are likely seeing two distinct 'humps'.\")\n",
    "    print(\"1. A large hump on the left (e.g., 0-1 min): Gaps WITHIN a deployment.\")\n",
    "    print(\"2. A smaller hump on the right (e.g., 20+ min): Gaps BETWEEN deployments.\")\n",
    "    print(\"\\nLook at the empty space between these two humps.\")\n",
    "    print(\"Pick a value (in minutes) from this empty space for the threshold in the next cell.\")\n",
    "    print(\"For example, if the first hump ends at 10 minutes and the next starts at 30, a good threshold is 20.\")\n",
    "else:\n",
    "    print(\"No timestamps found, skipping gap analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c673c",
   "metadata": {},
   "source": [
    "### Find Gaps and Identify Deployments\n",
    "\n",
    "**ACTION REQUIRED:** Look at the chart above and choose a threshold (in minutes) that separates the two gap clusters.\n",
    "Set the `GAP_THRESHOLD_MINUTES = XX` with your selected value. Using 10 minutes looks appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Gaps and Identify Deployments. This is the time between the last timestamp of one deployment and the first timestamp of the next.\n",
    "\n",
    "# --- !!! ACTION REQUIRED !!! ---\n",
    "# Based on the histogram in Cell 4, update this value.\n",
    "# For example, if the gap is at 10 minutes, set this to 10.\n",
    "GAP_THRESHOLD_MINUTES = 10 # <--- CHANGE THIS VALUE\n",
    "GAP_THRESHOLD = pd.Timedelta(minutes=GAP_THRESHOLD_MINUTES)\n",
    "# -----------------------------\n",
    "\n",
    "print(f\"Identifying deployments using a gap threshold of {GAP_THRESHOLD_MINUTES} minutes...\")\n",
    "\n",
    "# 1. Create a DataFrame of all images\n",
    "image_deployments = pd.DataFrame({'timestamp': all_timestamps_series})\n",
    "\n",
    "# 2. Calculate the time gap between each consecutive image\n",
    "gaps = image_deployments['timestamp'].diff()\n",
    "\n",
    "# 3. Find where the gap is larger than our threshold\n",
    "is_new_deployment = (gaps > GAP_THRESHOLD)\n",
    "\n",
    "# 4. Create a \"deployment_id\"\n",
    "image_deployments['deployment_id'] = is_new_deployment.cumsum()\n",
    "\n",
    "# 5. Get the start and end time for each deployment\n",
    "deployment_stats = image_deployments.groupby('deployment_id')['timestamp'].agg(\n",
    "    start_time='min',\n",
    "    end_time='max'\n",
    ")\n",
    "\n",
    "# 6. Calculate the duration of each deployment\n",
    "deployment_stats['duration_sec'] = (deployment_stats['end_time'] - deployment_stats['start_time']).dt.total_seconds()\n",
    "deployment_stats['deployDurMin'] = deployment_stats['duration_sec'] / 60.0\n",
    "\n",
    "print(f\"Identified {len(deployment_stats)} distinct deployments.\")\n",
    "print(\"\\nDeployment stats (first 10):\")\n",
    "print(deployment_stats.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594558b",
   "metadata": {},
   "source": [
    "### Match Events to Deployments and Get Durations\n",
    "\n",
    "This cell matches each CTD event to the deployment block it falls into.\n",
    "If you've chosen the correct threshold, you should now see a 1-to-1 mapping\n",
    "between events and deployments (i.e., no repeated `deployDurMin` values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321767f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match Events to Deployments\n",
    "# (This cell is unchanged, but its output will be correct now)\n",
    "\n",
    "print(\"\\n--- Matching CTD Events to Deployments ---\")\n",
    "\n",
    "# 1. Ensure both DataFrames are sorted by time for the merge\n",
    "data_sorted = data.sort_values('timeStamp')\n",
    "image_deployments_sorted = image_deployments.sort_values('timestamp')\n",
    "\n",
    "# 2. Perform the 'as-of' merge.\n",
    "# We set a tolerance so an event (CTD cast) can only match an image\n",
    "# that occurred recently. This prevents a CTD cast at 14:00 from\n",
    "# matching a deployment that ended at 12:00.\n",
    "# We'll use a 20-minute tolerance (the 10-min before, 10-min after logic)\n",
    "merge_tolerance = pd.Timedelta(minutes=20)\n",
    "\n",
    "merged_data = pd.merge_asof(\n",
    "    data_sorted,\n",
    "    image_deployments_sorted,\n",
    "    left_on='timeStamp',\n",
    "    right_on='timestamp',\n",
    "    direction='nearest', # Find the *closest* image to the CTD cast\n",
    "    tolerance=merge_tolerance\n",
    ")\n",
    "\n",
    "# 3. Join the duration stats\n",
    "#    Now we merge in the duration for the deployment_id we found.\n",
    "final_data = pd.merge(\n",
    "    merged_data,\n",
    "    deployment_stats[['deployDurMin']],\n",
    "    on='deployment_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4. Sort back to the original index order\n",
    "final_data = final_data.sort_index()\n",
    "\n",
    "print(\"--- Matching Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0195751",
   "metadata": {},
   "source": [
    "### Cell 7: Review Final Results\n",
    "\n",
    "Review the final data. If you still see repeated `deployDurMin` values,\n",
    "go back to **Cell 5** and try a *smaller* `GAP_THRESHOLD_MINUTES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Review Final Results\n",
    "# (I've added sorting to make groups easier to see)\n",
    "\n",
    "print(\"DataFrame with final calculated deployment durations:\")\n",
    "\n",
    "# Sort by deployment_id to make groups easy to see\n",
    "final_data_sorted_for_review = final_data.sort_values(by=['deployment_id', 'timeStamp'])\n",
    "\n",
    "# Print the relevant columns\n",
    "print(final_data_sorted_for_review[[\n",
    "    'timeStamp', \n",
    "    'deployment_id', \n",
    "    'deployDurMin',\n",
    "    'year', \n",
    "    'month', \n",
    "    'day', \n",
    "    'time_gmt'\n",
    "]].head(20)) # Print the first 20 rows\n",
    "\n",
    "# --- Optional: Save the updated DataFrame to a new CSV file ---\n",
    "output_filename = 'ctd_meta_with_durations.csv'\n",
    "final_data.to_csv(output_filename, index=False)\n",
    "print(f\"\\nSuccessfully saved updated data to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b6ca4",
   "metadata": {},
   "source": [
    "## --- Check sampling time distribution ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Distribution of Deployment Durations\n",
    "\n",
    "print(\"--- Analyzing Deployment Duration Distribution ---\")\n",
    "\n",
    "# We use the 'deployment_stats' DataFrame, which has one row per unique deployment.\n",
    "if 'deployment_stats' in locals() and not deployment_stats.empty:\n",
    "    \n",
    "    # --- 1. Print Descriptive Statistics ---\n",
    "    # This will immediately show you the min, max, and average durations.\n",
    "    print(\"Descriptive Statistics for Deployment Durations (in minutes):\")\n",
    "    print(deployment_stats['deployDurMin'].describe())\n",
    "    \n",
    "    # --- 2. Plot a Histogram ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # 'bins=50' is a good starting point; you can adjust this.\n",
    "    plt.hist(deployment_stats['deployDurMin'], bins=50, edgecolor='black')\n",
    "    \n",
    "    plt.title('Distribution of Deployment Durations', fontsize=16)\n",
    "    plt.xlabel('Deployment Duration (minutes)', fontsize=12)\n",
    "    plt.ylabel('Number of Deployments (Frequency)', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add a vertical line for the mean to see the skew\n",
    "    mean_duration = deployment_stats['deployDurMin'].mean()\n",
    "    plt.axvline(mean_duration, color='red', linestyle='dashed', linewidth=2)\n",
    "    plt.text(mean_duration * 1.05, plt.ylim()[1] * 0.9, f'Mean: {mean_duration:.2f} min', color='red')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Interpretation ---\")\n",
    "    print(\"Look for bars far to the right of the main cluster.\")\n",
    "    print(\"These represent your 'very long' deployments (the outliers).\")\n",
    "    print(\"The 'max' value in the statistics above is your longest deployment.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error: The 'deployment_stats' DataFrame was not found.\")\n",
    "    print(\"Please make sure you have successfully run 'Cell 5' (the 'Find Gaps and Identify Deployments' cell).\")\n",
    "\n",
    "# --- List Sampling Events with Long Deployments ---\n",
    "\n",
    "deploy_time = 20  # minutes\n",
    "print(f\"\\n\\n--- Identifying Sampling Events with Deployment Times >= {deploy_time} Minutes ---\")\n",
    "\n",
    "# We check for 'final_data', which links events to deployment stats\n",
    "if 'final_data' in locals() and not final_data.empty:\n",
    "    \n",
    "    # 1. Filter the 'final_data' DataFrame\n",
    "    long_deployments = final_data[final_data['deployDurMin'] >= deploy_time].copy()\n",
    "    \n",
    "    if long_deployments.empty:\n",
    "        print(f\"No sampling events found with deployment times >= {deploy_time} Minutes.\")\n",
    "    else:\n",
    "        print(f\"Found {len(long_deployments)} sampling events with deployment times >= {deploy_time} Minutes:\")\n",
    "        \n",
    "        # 2. Sort by duration (longest first) for easier review\n",
    "        long_deployments_sorted = long_deployments.sort_values(by='deployDurMin', ascending=False)\n",
    "        \n",
    "        # 3. Display the relevant info for these events\n",
    "        print(long_deployments_sorted[[\n",
    "            'timeStamp', \n",
    "            'Station',\n",
    "            'deployment_id', \n",
    "            'deployDurMin',\n",
    "            'year', \n",
    "            'month', \n",
    "            'day', \n",
    "            'time_gmt'\n",
    "        ]])\n",
    "        \n",
    "else:\n",
    "    print(\"Error: The 'final_data' DataFrame was not found.\")\n",
    "    print(\"Please make sure you have successfully run 'Cell 6' (the 'Match Events to Deployments' cell).\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b228507",
   "metadata": {},
   "source": [
    "### --- Cell for Testing: Calculate Duration for a Single Folder ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell for Testing: Find Deployments Within a Single Folder ---\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Set the exact path to the hourly folder you want to test\n",
    "TEST_FOLDER_PATH = 'cpics_img/20241110/20241110_0400' # <--- CHANGE THIS PATH\n",
    "\n",
    "# Set the gap threshold as requested\n",
    "GAP_THRESHOLD_MINUTES = 10\n",
    "GAP_THRESHOLD = pd.Timedelta(minutes=GAP_THRESHOLD_MINUTES)\n",
    "# -------------------------------------\n",
    "\n",
    "print(f\"--- Testing Folder: {TEST_FOLDER_PATH} ---\")\n",
    "print(f\"--- Using Gap Threshold: {GAP_THRESHOLD_MINUTES} minutes ---\")\n",
    "\n",
    "timestamps_in_folder = []\n",
    "duration_min = 0.0\n",
    "\n",
    "if not os.path.isdir(TEST_FOLDER_PATH):\n",
    "    print(f\"Error: Directory not found at '{TEST_FOLDER_PATH}'. Please check the path.\")\n",
    "else:\n",
    "    # Get all timestamps from the specified folder\n",
    "    for file in os.listdir(TEST_FOLDER_PATH):\n",
    "        if file.startswith('20') and file.endswith('.png'):\n",
    "            try:\n",
    "                dateStr = file[0:15]\n",
    "                file_timestamp = datetime.strptime(dateStr, '%Y%m%d_%H%M%S')\n",
    "                timestamps_in_folder.append(file_timestamp)\n",
    "            except ValueError:\n",
    "                print(f\"Skipping file with unexpected name format: {file}\")\n",
    "\n",
    "    if not timestamps_in_folder:\n",
    "        print(\"No valid .png files found in this folder.\")\n",
    "    else:\n",
    "        print(f\"Found {len(timestamps_in_folder)} total images in folder.\")\n",
    "        \n",
    "        # --- 2. Perform Gap Detection Logic ---\n",
    "        \n",
    "        # Create a DataFrame from the sorted list of timestamps\n",
    "        df = pd.DataFrame({'timestamp': sorted(timestamps_in_folder)})\n",
    "\n",
    "        # Calculate the time gap between each consecutive image\n",
    "        gaps = df['timestamp'].diff()\n",
    "\n",
    "        # Find where the gap is larger than our threshold\n",
    "        is_new_deployment = (gaps > GAP_THRESHOLD)\n",
    "\n",
    "        # Create a \"deployment_id\" for each block\n",
    "        df['deployment_id'] = is_new_deployment.cumsum()\n",
    "\n",
    "        # --- 3. Group by Deployment ID and Calculate Stats ---\n",
    "        deployment_stats = df.groupby('deployment_id')['timestamp'].agg(\n",
    "            start_time='min',\n",
    "            end_time='max',\n",
    "            image_count='count'\n",
    "        )\n",
    "\n",
    "        # Calculate the duration (in minutes) for each deployment\n",
    "        deployment_stats['duration_sec'] = (deployment_stats['end_time'] - deployment_stats['start_time']).dt.total_seconds()\n",
    "        deployment_stats['duration_min'] = deployment_stats['duration_sec'] / 60.0\n",
    "\n",
    "        # --- 4. Report the Results ---\n",
    "        num_deployments = len(deployment_stats)\n",
    "        print(f\"\\nFound {num_deployments} distinct deployment(s) in this folder:\")\n",
    "        \n",
    "        # Re-order columns for a nice printout\n",
    "        final_stats = deployment_stats[['start_time', 'end_time', 'duration_min', 'image_count']]\n",
    "        print(final_stats)\n",
    "\n",
    "print(\"\\n--- Test Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e80e23",
   "metadata": {},
   "source": [
    "## NOTES:\n",
    "\n",
    "### Deployment time for BG18 during WS24205 on 2024-07-26 at 20:18 was set manually to 40.55 min because the corresponding folder in 'cpics_img' contains pngs that appear to come from separate deployments because of the long cast. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
